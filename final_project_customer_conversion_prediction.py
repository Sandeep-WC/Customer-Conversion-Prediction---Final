# -*- coding: utf-8 -*-
"""Final Project - Customer Conversion Prediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IDeOoEIO_i5UiCk3QEsQea2956fDDlN1

#Customer Conversion Prediction

Problem Statement
You are working for a new-age insurance company and employ mutiple outreach plans to sell term insurance to your customers. Telephonic marketing campaigns still remain one of the most effective way to reach out to people however they incur a lot of cost. Hence, it is important to identify the customers that are most likely to convert beforehand so that they can be specifically targeted via call. We are given the historical marketing data of the insurance company and are required to build a ML model that will predict if a client will subscribe to the insurance. 

Data
The historical sales data is available as a compressed file here. 

Features: 
•	age (numeric)
•	job : type of job
•	marital : marital status
•	educational_qual : education status
•	call_type : contact communication type
•	day: last contact day of the month (numeric)
•	mon: last contact month of year
•	dur: last contact duration, in seconds (numeric)
•	num_calls: number of contacts performed during this campaign and for this client 
•	prev_outcome: outcome of the previous marketing campaign (categorical: "unknown","other","failure","success")
Output variable (desired target):
•	y - has the client subscribed to the insurance?

Minimum Requirements
It is not sufficient to just fit a model - the model must be analysed to find the important factors that contribute towards the conversion rate. AUROC must be used as a metric to evaluate the performance of the models.

**IMPORTING THE REQUIRED LIABARIES**
"""

import pandas as pd # handle the data
import numpy as np #handle the arrays
from sklearn.model_selection import train_test_split # split the data

from sklearn import metrics # evaluation metrics
import seaborn as sns # plotting the data
import matplotlib.pyplot as plt # plotting the data
import warnings 
warnings.filterwarnings('ignore')

dataset = pd.read_csv('/content/train.csv')
dataset

"""The ten features we’ll be using are:

1. age (numeric) 
2. job : type of job 
3. marital : marital status 
4. educational_qual : education status 
5. call_type : contact communication type 
6. day: last contact day of the month (numeric) 
7. mon: last contact month of year 
8. dur: last contact duration, in seconds (numeric) 
9. num_calls: number of contacts performed during this campaign and for this client 
10. prev_outcome: outcome of the previous marketing campaign (categorical: "unknown","other","failure","success") Output variable (desired target): 
11. y - has the client subscribed to the insurance?

#DATA JAR
**CLEAN**
"""

dataset.shape

dataset.isnull().sum()

dataset=dataset.drop_duplicates() #dropping duplicates if any
dataset.shape

dataset.describe()

iqr = dataset['age'].quantile(0.75) - dataset['age'].quantile(0.25)
upper_threshold = dataset['age'].quantile(0.75) + (1.5 * iqr)
lower_threshold = dataset['age'].quantile(0.25) - (1.5 * iqr)
upper_threshold, lower_threshold
#Though there is an outlier in age, we will not treat the outlier as bad, as all the outliers are not bad.

iqr = dataset['day'].quantile(0.75) - dataset['day'].quantile(0.25)
upper_threshold = dataset['day'].quantile(0.75) + (1.5 * iqr)
lower_threshold = dataset['day'].quantile(0.25) - (1.5 * iqr)
upper_threshold, lower_threshold

iqr = dataset['dur'].quantile(0.75) - dataset['dur'].quantile(0.25)
upper_threshold = dataset['dur'].quantile(0.75) + (1.5 * iqr)
lower_threshold = dataset['dur'].quantile(0.25) - (1.5 * iqr)
upper_threshold, lower_threshold

dataset.dur=dataset.dur.clip(-221.0,643.0) 
#Clipping the duration column as the upper threshold is high, which definitely affects the model.

iqr = dataset['num_calls'].quantile(0.75) - dataset['num_calls'].quantile(0.25)
upper_threshold = dataset['num_calls'].quantile(0.75) + (1.5 * iqr)
lower_threshold = dataset['num_calls'].quantile(0.25) - (1.5 * iqr)
upper_threshold, lower_threshold

dataset.num_calls=dataset.num_calls.clip(-2.0,6.0) 
#Clipping the num_calls column as the upper threshold is high, which definitely affects the model.

dataset.describe()

dataset.dtypes

"""#DATA ANALYSIS AND VISUALIZATION
Before we start analyzing the data, we will first code the output variable, and, which has information about whether a customer has purchased or subscribed to a term deposit, with numerical values. You can use the following code to make an encoding of the output variable, and, with zeros and ones:
"""

dataset['conversion'] = dataset['y'].apply(lambda x: 0 if x == 'no' else 1)

"""#EDA (Conversion rate)
Let's first take a look at the aggregate conversion rate. The conversion rate is simply the percentage of customers who got converted
"""

conversion_rate = pd.DataFrame(
    dataset.groupby('conversion').count()['y'] / dataset.shape[0] * 100.0)
conversion_rate.T

dataset.conversion.value_counts().plot(kind='bar', title='Count (conversion)');

"""As you can see, only about 11.7% were converted. From these results, we can see that there is a large imbalance between the conversion group and the non-conversion group, which is common and often seen between various marketing data sets.

**CONVERSION RATE BY JOB**
"""

dataset['job'].value_counts()

dataset['job']=dataset['job'].replace('unknown',dataset['job'].mode()[0])

dataset['job'].value_counts()

dataset['job'].unique()

conversion_rate_by_job = dataset.groupby(
    by='job'
)['conversion'].sum() / dataset.groupby(
    by='job'
)['conversion'].count()*100.0

conversion_rate_by_job #Because numbers are difficult to visualize, we will plot the data for better comprehension.

(dataset.groupby('job')['conversion'].mean()*100).sort_values().plot(kind='barh')

"""**OBSERVATIONS**: 
- We can easily see that the student and retiree groups are the two groups with the highest conversion rates, while the worker and employer groups are the two groups with the lowest conversion rates. 
- It is also observed that the unknown values are 288 out of 45205, so we imputed them with mode. 
- By the count, it is clear that a salesperson is putting in a lot of effort in a blue-collar job with a low conversion rate. It might be due to the low wages, but they are the ones who need it the most. For greater conversion, the emphasis can be shifted to other groups, such as students and the retired.
- This can be label encoded, as the order based on rank is visible in the plot.

**CONVERSION RATE BY MARITAL**
"""

dataset['marital'].value_counts()

conversion_rate_by_marital = dataset.groupby(
    by='marital'
)['conversion'].sum() / dataset.groupby(
    by='marital'
)['conversion'].count()*100.0

conversion_rate_by_marital

(dataset.groupby('marital')['conversion'].mean()*100).sort_values().plot(kind='bar')

"""**OBSERVATIONS**:
- Singles are taking out more insurance compared to divorced and married people; this could be because the younger you are, the lower the premium.
- This can be label encoded, as the order based on rank is visible in the plot

**CONVERSION RATE BY EDUCATION QAULIFICATION**
"""

dataset['education_qual'].value_counts()

dataset['education_qual']=dataset['education_qual'].replace('unknown',dataset['education_qual'].mode()[0])

dataset['education_qual'].value_counts()

conversion_rate_by_education_qual= dataset.groupby(
    by='education_qual'
)['conversion'].sum()/ dataset.groupby(
    by='education_qual'
)['conversion'].count()*100.0

conversion_rate_by_education_qual

(dataset.groupby('education_qual')['conversion'].mean()*100).sort_values().plot(kind='bar')

"""**OBSERVATIONS**: 
- We can easily see that the conversion rate is proportional to education; as education increases, the conversion rate increases, and this is obvious.
- It is also observed that the unknown values are 1857 out of 45205, so we imputed them with mode.
- This can be label encoded, as the order based on rank is visible in the plot.

**CONVERSION RATE BY CALL TYPE**
"""

dataset['call_type'].value_counts()

conversion_rate_by_call_type= dataset.groupby(
    by='call_type'
)['conversion'].sum()/dataset.groupby(
    by='call_type'
)['conversion'].count()*100.0

conversion_rate_by_call_type

(dataset.groupby('call_type')['conversion'].mean()*100).sort_values().plot(kind='bar')

"""**OBSERVATIONS**: 
- Cell 65% is clearly the most frequently used tool here. It could be because people decline the call made by telephone due to a pattern of thinking of it as a sales call.
- There are 13017 unknown values observed, which are huge in quantity, so rather than imputing them, we will treat them as a category.
- This can be label encoded, as the order based on rank is visible in the plot.

**CONVERSION RATE BY MONTH**
"""

dataset['mon'].value_counts()

conversion_rate_by_mon= dataset.groupby(
    by='mon'
)['conversion'].sum() / dataset.groupby(
    by='mon'
)['conversion'].count()*100.0

conversion_rate_by_mon

(dataset.groupby('mon')['conversion'].mean()*100).sort_values().plot(kind='barh')

"""**OBSERVATIONS**: 
- The conversion rate is clearly higher in March; this could be due to the end of the fiscal year. 
- Although fewer people are contacted in December, September, and October than in March, the conversion rate remains high. 
- This can be label encoded, as the order based on rank is visible in the plot.

**CONVERSION RATE BY PREVIOUS OUTCOME**
"""

dataset['prev_outcome'].value_counts()

conversion_rate_by_prev_outcome= dataset.groupby(
    by='prev_outcome'
)['conversion'].sum()/dataset.groupby(
    by='prev_outcome'
)['conversion'].count()*100.0

conversion_rate_by_prev_outcome

(dataset.groupby('prev_outcome')['conversion'].mean()*100).sort_values().plot(kind='barh')

"""**OBSERVATIONS**: 
- The graph shows that previous outcomes have a relationship with the result; if the previous outcome was success, the client is more likely to buy insurance.
- Almost 70% of the data is unknown, so imputing makes no sense. 
Instead of removing the feature, treat unknown values as categories.
- This can be label encoded, as the order based on rank is visible in the plot.

**CONVERSION RATE BY AGE**
"""

conversion_rate_by_age= dataset.groupby(
    by='age'
)['conversion'].sum()/dataset.groupby(
    by='age'
)['conversion'].count()*100.0

conversion_rate_by_age

sns.boxplot(data=dataset, x="y", y="age")

"""**OBSERVATIONS**: 
- In this plot, we can clearly see that people with higher ages are more likely to buy insurance.

**CONVERSION RATE BY DAY**
"""

conversion_rate_by_day= dataset.groupby(
    by='day'
)['conversion'].sum()/dataset.groupby(
    by='day'
)['conversion'].count()*100.0

conversion_rate_by_day

(dataset.groupby('day')['conversion'].mean()*100).sort_values().plot(kind='barh', color='orange')

"""**OBSERVATIONS**: 
- In this plot, we can clearly see that engagement is high, with people purchasing more insurance in the beginning, middle, and end of the month.

**CONVERSION RATE BY DURATION**
"""

conversion_rate_by_dur= dataset.groupby(
    by='dur'
)['conversion'].sum()/dataset.groupby(
    by='dur'
)['conversion'].count()*100.0

conversion_rate_by_dur

sns.boxplot(data=dataset, x="y", y="dur")

"""**OBSERVATIONS**: 
- It is self-evident that the length of the call increases the likelihood of the client purchasing insurance.

**CONVERSION RATE BY NO.OF CALLS**
"""

conversion_rate_by_num_calls= dataset.groupby(
    by='num_calls'
)['conversion'].sum()/dataset.groupby(
    by='num_calls'
)['conversion'].count()*100.0

conversion_rate_by_num_calls

sns.boxplot(data=dataset, x="y", y="num_calls")

"""**OBSERVATIONS**: 
- The distribution is similar for both

#ENCODING
There are six categorical variables in this data set: job, marital, education_qual,call type, month, and previous outcome. Before we start building ML models, we need to code these categorical variables with numerical values. As decided in EDA, there is an order to the data, so all will be label encoded.

Checking for the labels in the categorical parameters
"""

print(dataset['job'].unique()) 
print(dataset['marital'].unique())
print(dataset['education_qual'].unique())
print(dataset['call_type'].unique()) 
print(dataset['mon'].unique()) 
print(dataset['prev_outcome'].unique()) 
print(dataset['y'].unique())

dataset['job'] = dataset['job'].map({'blue-collar':0, 'entrepreneur':1, 'housemaid':2, 'services':3, 'technician':4, 
                                     'self-employed':5, 'admin.':6, 'management':7, 'unemployed':8, 'retired':9, 'student':10})

dataset['marital'] = dataset['marital'].map({"married" : 0 , "divorced" : 1 , "single" : 2})

dataset['education_qual'] = dataset['education_qual'].map({"primary" : 0 , "secondary" : 1 , "tertiary" : 2})

dataset['call_type'] = dataset['call_type'].map({"unknown" : 0 , "telephone" : 1 , "cellular" : 2})

dataset['mon'] = dataset['mon'].map({"may" : 0 , "jul" : 1 , "jan" : 2 , "nov" : 3 , 
                           "jun" : 4 , "aug" : 5 , "feb" : 6 , "apr" : 7 , "oct" : 8 , 
                           "sep" : 9 , "dec" : 10, "mar":11})

dataset['prev_outcome'] = dataset['prev_outcome'].map({"unknown" : 0 , "failure" : 1 , "other" : 2, "success": 3})

dataset.head()

dataset.isnull().sum()

"""#SPLIT"""

dataset.columns

X = dataset[['age', 'job', 'marital', 'education_qual', 'call_type', 'day', 'mon',
       'dur', 'num_calls', 'prev_outcome']].values #array of features
y = dataset['conversion'].values #array of targets

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=0) # train- test split

X_train.shape, X_test.shape

"""#Data Balancing"""

!pip install imblearn

import imblearn

dataset['job'].unique()

dataset.shape

from imblearn.combine import SMOTEENN
smt = SMOTEENN(sampling_strategy='all')
X_smt, y_smt = smt.fit_resample(X_train, y_train)

X_smt.shape

y_smt.shape

"""#SCALING"""

from sklearn.preprocessing import StandardScaler ## standrard scalig  
scaler = StandardScaler() #initialise to a variable
scaler.fit(X_smt) # we are finding the values of mean and sd from the td
X_train_scaled = scaler.transform(X_smt) # fit (mean, sd) and then transform the training data
X_test_scaled = scaler.transform(X_test) # transform the test data

"""#MODEL TRAINING

**LOGISTIC REGRESSION**
"""

from sklearn.linear_model import LogisticRegression #main code that build the LR model 
from sklearn.metrics import roc_auc_score #for evaluation
lr= LogisticRegression() #initialise the required package
lr.fit(X_train_scaled, y_smt) #magic happens - best values of betas - training/learning happens here
y_pred=lr.predict(X_test_scaled)

#logistic_regression.coef_
y_pred

lr.predict_proba(X_test_scaled)

from sklearn.metrics import roc_auc_score
log = roc_auc_score(y_test, lr.predict_proba(X_test_scaled)[:, 1]) 
log

"""**KNN**"""

from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
knn.fit(X_train_scaled,y_smt)
knn.score(X_test_scaled,y_test)

for i in [1,2,3,4,5,6,7,8,9,10,20,30,40,50]:
  knn = KNeighborsClassifier(i) #initialising the model
  knn.fit(X_train_scaled,y_smt) # training the model
  print("K value  : " , i, " train score : ", knn.score(X_train_scaled,y_smt) , 
        " cv score : ", np.mean(cross_val_score(knn, X_train_scaled, y_smt, cv=10, scoring = "roc_auc"))) #predicting using the model

knn = KNeighborsClassifier(n_neighbors=7)
knn.fit(X_train_scaled,y_smt)
knn.score(X_test_scaled,y_test)
K = roc_auc_score(y_test, knn.predict_proba(X_test_scaled)[:,1])
K

"""**DECISION TREE**"""

from sklearn.tree import DecisionTreeClassifier 
dt = DecisionTreeClassifier()
dt.fit(X_train_scaled, y_smt)
dt.score(X_test_scaled, y_test)

for i in [1,2,3,4,5,6,7,8,9,10,11,12,20,30,40,50]:
  dt = KNeighborsClassifier(i) #initialising the model
  dt.fit(X_train_scaled,y_smt) # training the model
  print("depth  : " , i, " train score : ", dt.score(X_train_scaled,y_smt) , 
        " cv score : ", np.mean(cross_val_score(dt, X_train_scaled, y_smt, cv=10, scoring = "roc_auc"))) #predicting using the model

dt = DecisionTreeClassifier(max_depth=10)
dt.fit(X_train_scaled, y_smt)
dt.score(X_test_scaled, y_test) 
d = roc_auc_score(y_test, dt.predict_proba(X_test_scaled)[:,1])
d

from sklearn import tree
tree.plot_tree(dt) # dt is the decison tree that I have learnt

"""**RANDOM FOREST**"""

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators= 100, max_depth = 4, max_features='sqrt')
rf.fit(X_train_scaled, y_smt)
rf.score(X_train_scaled, y_smt)

r = roc_auc_score(y_test, rf.predict_proba(X_test_scaled)[:,1])
r

"""**XGBOOST**"""

import xgboost as xgb
from sklearn.model_selection import cross_val_score
import numpy as np
for lr in [0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.2,0.21,0.22,0.23,0.24,0.25,0.5,0.7,1]:
  model = xgb.XGBClassifier(learning_rate = lr, n_estimators=100, verbosity = 0) # initialise the model
  model.fit(X_train_scaled,y_smt) #train the model
  model.score(X_test_scaled, y_test) # scoring the model 
  print("Learning rate : ", lr, " Train score : ", model.score(X_train_scaled,y_smt), " Cross-Val score : ", np.mean(cross_val_score(model, X_train_scaled, y_smt, cv=10)))

model = xgb.XGBClassifier(learning_rate = 0.25, n_estimators=100)
model.fit(X_train_scaled,y_smt) #train the model
model.score(X_train_scaled,y_smt) # scoring the model

xg = roc_auc_score(y_test, model.predict_proba(X_test_scaled)[:,1])
xg

pd.DataFrame({'Model':['Logistic Regression', 'KNN', 'Decision Tree', 'Random Forest', 'XGBoost'], 'Auroc':[log, K, d, r, xg]})

"""**FEATURE IMPORTANCE**"""

imp_ft = pd.DataFrame({'ft':['age', 'job', 'marital', 'education_qual', 'call_type', 'day', 'mon','dur', 'num_calls', 'prev_outcome'],
                       'imp':model.feature_importances_})
imp_ft.sort_values('imp', ascending=False, inplace=True)

imp_ft.iloc[0:5,0].values

X_imp= dataset.loc[:, imp_ft.iloc[0:5,0]].values
y = dataset['conversion'].values

from sklearn.model_selection import train_test_split
X_train_imp,X_test_imp,y_train,y_test = train_test_split(X_imp,y,test_size=0.25,random_state=0) # train- test split

from imblearn.combine import SMOTEENN
smt = SMOTEENN(sampling_strategy='all')
X_smt_imp, y_smt = smt.fit_resample(X_train_imp, y_train)

from sklearn.preprocessing import StandardScaler ## standrard scalig  
scaler = StandardScaler() #initialise to a variable
scaler.fit(X_smt_imp) # we are finding the values of mean and sd from the td
X_train_imp_scaled = scaler.transform(X_smt_imp) # fit (mean, sd) and then transform the training data
X_test_imp_scaled = scaler.transform(X_test_imp) # transform the test data

import xgboost as xgb
from sklearn.model_selection import cross_val_score
import numpy as np
for lr in [0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.2,0.21,0.22,0.23,0.24,0.25,0.5,0.7,1]:
  model_imp = xgb.XGBClassifier(learning_rate = lr, n_estimators=100, verbosity = 0) # initialise the model
  model_imp.fit(X_train_imp_scaled,y_smt) #train the model
  model_imp.score(X_test_imp_scaled, y_test) # scoring the model 
  print("Learning rate : ", lr, " Train score : ", model_imp.score(X_train_imp_scaled,y_smt), " Cross-Val score : ", np.mean(cross_val_score(model_imp, X_train_imp_scaled, y_smt, cv=10)))

model_imp = xgb.XGBClassifier(learning_rate = 0.24, n_estimators=100)
model_imp.fit(X_train_imp_scaled,y_smt) #train the model
model_imp.score(X_train_imp_scaled,y_smt) # scoring the model

xg = roc_auc_score(y_test, model_imp.predict_proba(X_test_imp_scaled)[:,1])
xg

"""#Conclusion 
- After applying all the models, XG boost has a higher score of 0.92, which is an excellent Auroc score.
"""